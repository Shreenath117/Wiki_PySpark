{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.io._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.io._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.mapreduce.Job\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.mapreduce.Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.conf.Configuration\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.conf.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext\r\n",
       "import org.apache.spark.SparkConf\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\r",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r",
      "java.lang.reflect.Constructor.newInstance(Unknown Source)\r",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r",
      "py4j.Gateway.invoke(Gateway.java:238)\r",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\r",
      "java.lang.Thread.run(Unknown Source)\r",
      "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2456)\r",
      "  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2452)\r",
      "  at scala.Option.foreach(Option.scala:257)\r",
      "  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2452)\r",
      "  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2541)\r",
      "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:84)\r",
      "  ... 39 elided",
      ""
     ]
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"Project\").setMaster(\"local[*]\")\n",
    "new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Shariq:4040\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = local[*], app id = local-1586003975035)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\r\n",
       "import org.apache.hadoop.io.LongWritable\r\n",
       "import org.apache.hadoop.io.Text\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n",
    "import org.apache.hadoop.io.LongWritable\n",
    "import org.apache.hadoop.io.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, __spark_hadoop_conf__.xml\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"textinputformat.record.delimiter\",\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wikilinks: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010 NewHadoopRDD[0] at newAPIHadoopFile at <console>:30\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wikilinks = sc.newAPIHadoopFile(\"C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010\",\n",
    "                                   classOf[TextInputFormat], classOf[LongWritable],classOf[Text],conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at map at <console>:34\r\n",
       "res11: Array[String] = Array(0, 372, 717)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = wikilinks.map(tup => (tup._1.toString))\n",
    "d.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: identifier expected but integer literal found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: identifier expected but integer literal found.\r",
      "val y = wikilinks.mapPartitions(lambda x: (x[2]))\r",
      "                                             ^",
      ""
     ]
    }
   ],
   "source": [
    "val y = wikilinks.mapPartitions(lambda x: (x[2]))\n",
    "y\n",
    "// transactionDataRdd.map(lambda x: (x[2],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\n",
      "MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter\n",
      "TOKEN\tbluring\t905997\n",
      "TOKEN\twell\t863642\n",
      "TOKEN\tmakes\t863424\n",
      "TOKEN\tDarkKhaki\t837410\n",
      "TOKEN\tuser\t863302\n",
      "TOKEN\tenumeration\t923873\n",
      "TOKEN\telemement\t849381\n",
      "TOKEN\tgives\t847098\n",
      "TOKEN\tdiffR\t915725\n",
      "TOKEN\tskip\t846891"
     ]
    }
   ],
   "source": [
    "val e = wikilinks.map(tup => (tup._2.toString))\n",
    "val test = e.foreach( println)\n",
    "// test.getClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xz: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at map at <console>:34\r\n",
       "res14: String =\r\n",
       "URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\r\n",
       "MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter\r\n",
       "TOKEN\tbluring\t905997\r\n",
       "TOKEN\twell\t863642\r\n",
       "TOKEN\tmakes\t863424\r\n",
       "TOKEN\tDarkKhaki\t837410\r\n",
       "TOKEN\tuser\t863302\r\n",
       "TOKEN\tenumeration\t923873\r\n",
       "TOKEN\telemement\t849381\r\n",
       "TOKEN\tgives\t847098\r\n",
       "TOKEN\tdiffR\t915725\r\n",
       "TOKEN\tskip\t846891\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val xz = wikilinks.map(ele => ele._2.toString)\n",
    "xz.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 1089243\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xz.filter(line => line.contains(\"URL\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, Iterable[org.apache.hadoop.io.Text])] = ShuffledRDD[13] at groupByKey at <console>:34\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yup: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[195] at map at <console>:37\r\n",
       "nDF: org.apache.spark.sql.DataFrame = [value: string]\r\n",
       "res93: org.apache.spark.sql.Row =\r\n",
       "[URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\r\n",
       "MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter\r\n",
       "TOKEN\tbluring\t905997\r\n",
       "TOKEN\twell\t863642\r\n",
       "TOKEN\tmakes\t863424\r\n",
       "TOKEN\tDarkKhaki\t837410\r\n",
       "TOKEN\tuser\t863302\r\n",
       "TOKEN\tenumeration\t923873\r\n",
       "TOKEN\telemement\t849381\r\n",
       "TOKEN\tgives\t847098\r\n",
       "TOKEN\tdiffR\t915725\r\n",
       "TOKEN\tskip\t846891]\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yup = wikilinks.map(ele => ele._2.toString)\n",
    "val nDF = yup.map(ele => ele.toString).toDF\n",
    "nDF.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yup: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[157] at map at <console>:37\r\n",
       "fDF: org.apache.spark.sql.DataFrame = [_1: string, _2: string]\r\n",
       "res78: org.apache.spark.sql.Row =\r\n",
       "[URL\t,ftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\r\n",
       "MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter\r\n",
       "TOKEN\tbluring\t905997\r\n",
       "TOKEN\twell\t863642\r\n",
       "TOKEN\tmakes\t863424\r\n",
       "TOKEN\tDarkKhaki\t837410\r\n",
       "TOKEN\tuser\t863302\r\n",
       "TOKEN\tenumeration\t923873\r\n",
       "TOKEN\telemement\t849381\r\n",
       "TOKEN\tgives\t847098\r\n",
       "TOKEN\tdiffR\t915725\r\n",
       "TOKEN\tskip\t846891]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yup = wikilinks.map(ele => ele._2.toString)\n",
    "val fDF = yup.map(ele => ele.splitAt(4)).toDF\n",
    "fDF.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  _1|                  _2|\n",
      "+----+--------------------+\n",
      "|URL\t|ftp://194.44.98.4...|\n",
      "|\n",
      "URL|\tftp://202.38.89....|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _2|\n",
      "+--------------------+\n",
      "|ftp://194.44.98.4...|\n",
      "|\tftp://202.38.89....|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDF.select(\"_2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|URL\tftp://194.44....|\n",
      "|\n",
      "URL\tftp://202.38...|\n",
      "|\n",
      "URL\tftp://212.15...|\n",
      "|\n",
      "URL\tftp://216.24...|\n",
      "|\n",
      "URL\tftp://auburn...|\n",
      "|\n",
      "URL\tftp://db.sta...|\n",
      "|\n",
      "URL\tftp://disk.k...|\n",
      "|\n",
      "URL\tftp://dlink....|\n",
      "|\n",
      "URL\tftp://earthb...|\n",
      "|\n",
      "URL\tftp://fedora...|\n",
      "|\n",
      "URL\tftp://ftp.at...|\n",
      "|\n",
      "URL\tftp://ftp.ce...|\n",
      "|\n",
      "URL\tftp://ftp.do...|\n",
      "|\n",
      "URL\tftp://ftp.do...|\n",
      "|\n",
      "URL\tftp://ftp.em...|\n",
      "|\n",
      "URL\tftp://ftp.eu...|\n",
      "|\n",
      "URL\tftp://ftp.fa...|\n",
      "|\n",
      "URL\tftp://ftp.fa...|\n",
      "|\n",
      "URL\tftp://ftp.fi...|\n",
      "|\n",
      "URL\tftp://ftp.ge...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nDF.select(\"value\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yup: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[224] at map at <console>:37\r\n",
       "newLineDF: org.apache.spark.sql.DataFrame = [value: array<string>]\r\n",
       "res101: org.apache.spark.sql.Row = [WrappedArray(URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml, MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter, TOKEN\tbluring\t905997, TOKEN\twell\t863642, TOKEN\tmakes\t863424, TOKEN\tDarkKhaki\t837410, TOKEN\tuser\t863302, TOKEN\tenumeration\t923873, TOKEN\telemement\t849381, TOKEN\tgives\t847098, TOKEN\tdiffR\t915725, TOKEN\tskip\t846891)]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yup = wikilinks.map(ele => ele._2.toString)\n",
    "val newLineDF = yup.map(ele => ele.split(\"\\n\")).toDF\n",
    "newLineDF.take(1).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yup: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[234] at map at <console>:37\r\n",
       "newLineDF: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[235] at map at <console>:38\r\n",
       "res105: Array[Array[String]] =\r\n",
       "Array(Array(URL, ftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\r\n",
       "MENTION, Bayer filter, 863044, http://en.wikipedia.org/wiki/Bayer_filter\r\n",
       "TOKEN, bluring, 905997\r\n",
       "TOKEN, well, 863642\r\n",
       "TOKEN, makes, 863424\r\n",
       "TOKEN, DarkKhaki, 837410\r\n",
       "TOKEN, user, 863302\r\n",
       "TOKEN, enumeration, 923873\r\n",
       "TOKEN, elemement, 849381\r\n",
       "TOKEN, gives, 847098\r\n",
       "TOKEN, diffR, 915725\r\n",
       "TOKEN, skip, 846891), Array(\"\r\n",
       "URL\", ftp://202.38.89.18/incoming/ASME/data/pdfs/trk-18/IMECE2011-63321.pdf\r\n",
       "MENTION, starter, 14667, http://en.wikipedia.org/wiki/Starter_motor\r\n",
       "TOKEN..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yup = wikilinks.map(ele => ele._2.toString)\n",
    "val newLineDF = yup.map(ele => ele.split(\"\\t\"))\n",
    "newLineDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res131: Array[String] = Array(URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml, \"\")\n"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yup.map { r =>\n",
    "  val t = r.split(\"\\n\")\n",
    "  (t(0).trim())\n",
    "}.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newItem: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newItem = yup.map { r =>\n",
    "  val t = r.replaceAll(\"\\n\\n\",\"\\n\")split(\"\\n\")\n",
    "  (t(0).trim(),t(1).trim(),t(2).trim(),t(3).trim())\n",
    "}.toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newItem.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 01:29:24 ERROR Executor:91 - Exception in task 12.0 in stage 94.0 (TID 194)\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "2020-04-05 01:29:24 WARN  TaskSetManager:66 - Lost task 12.0 in stage 94.0 (TID 194, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "2020-04-05 01:29:24 ERROR TaskSetManager:70 - Task 12 in stage 94.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 12 in stage 94.0 failed 1 times, most recent failure: Lost task 12.0 in stage 94.0 (TID 194, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 94.0 failed 1 times, most recent failure: Lost task 12.0 in stage 94.0 (TID 194, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r",
      "  at scala.Option.foreach(Option.scala:257)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\r",
      "  ... 36 elided\r",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException",
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 11:54:15 WARN  Executor:87 - Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:785)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:814)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:814)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:814)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(Unknown Source)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 14 more\n",
      "2020-04-05 11:54:16 WARN  NettyRpcEnv:66 - Ignored message: HeartbeatResponse(false)\n"
     ]
    }
   ],
   "source": [
    "newItem.filter($\"_4\" === \"MENTION\").select($\"_1\",$\"_2\".alias(\"Date Of Birth\"),$\"_4\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _4|\n",
      "+--------------------+\n",
      "|   TOKEN\twell\t863642|\n",
      "|TOKEN\tposition\t19094|\n",
      "|MENTION\tpsycholog...|\n",
      "|MENTION\tdigital s...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newItem.select(\"_4\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// here onwards DUMMY CODE, for hit and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yup: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[201] at map at <console>:37\r\n",
       "newLineDF: org.apache.spark.sql.DataFrame = [value: array<string>]\r\n",
       "res94: org.apache.spark.sql.Row =\r\n",
       "[WrappedArray(URL\tftp://194.44.98.42/UPLOAD/SOFT/delphi/AForge.NET%20Framework-2.2.3/Release/AForge.Imaging.xml\r\n",
       "MENTION\tBayer filter\t863044\thttp://en.wikipedia.org/wiki/Bayer_filter\r\n",
       "TOKEN\tbluring\t905997\r\n",
       "TOKEN\twell\t863642\r\n",
       "TOKEN\tmakes\t863424\r\n",
       "TOKEN\tDarkKhaki\t837410\r\n",
       "TOKEN\tuser\t863302\r\n",
       "TOKEN\tenumeration\t923873\r\n",
       "TOKEN\telemement\t849381\r\n",
       "TOKEN\tgives\t847098\r\n",
       "TOKEN\tdiffR\t915725\r\n",
       "TOKEN\tskip\t846891)]\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yup = wikilinks.map(ele => ele._2.toString)\n",
    "val newLineDF = yup.map(ele => ele.split(\"\\n\\n\")).toDF\n",
    "newLineDF.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newLineDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|URL\tftp://194.44....|\n",
      "|\n",
      "URL\tftp://202.38...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newLineDF.withColumn(\"value\",explode($\"value\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[URL\tftp://194.44...|\n",
      "|[\n",
      "URL\tftp://202.3...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newLineDF.select(\"value\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[URL\tftp://194.44...|\n",
      "|[, URL\tftp://202....|\n",
      "|[, URL\tftp://212....|\n",
      "|[, URL\tftp://216....|\n",
      "|[, URL\tftp://aubu...|\n",
      "|[, URL\tftp://db.s...|\n",
      "|[, URL\tftp://disk...|\n",
      "|[, URL\tftp://dlin...|\n",
      "|[, URL\tftp://eart...|\n",
      "|[, URL\tftp://fedo...|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "|[, URL\tftp://ftp....|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, Row, SQLContext}\r\n",
       "aliasAllColumns: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{ DataFrame, Row, SQLContext }\n",
    "def aliasAllColumns(df: DataFrame): DataFrame = {\n",
    "df.select(df.columns.map { c =>\n",
    "df.col(c)\n",
    ".as(\n",
    "c.replaceAll(\"_\", \"\")\n",
    ".replaceAll(\"([A-Z])\", \"_$1\")\n",
    ".toLowerCase\n",
    ".replaceFirst(\"_\", \"\"))\n",
    "}: _*)\n",
    "}\n",
    "aliasAllColumns(newLineDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Long = 1089244\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "29: error: object MyWritable is not a member of package org.apache.hadoop.io\r",
     "output_type": "error",
     "traceback": [
      "<console>:29: error: object MyWritable is not a member of package org.apache.hadoop.io\r",
      "       import org.apache.hadoop.io.MyWritable\r",
      "              ^",
      ""
     ]
    }
   ],
   "source": [
    "// import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n",
    "import org.apache.hadoop.io.MyWritable\n",
    "// import org.apache.hadoop.io.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 16:29:02 WARN  HadoopRDD:66 - Caching HadoopRDDs as deserialized objects usually leads to undesired behavior because Hadoop's RecordReader reuses the same Writable object for all records. Use a map transformation to make copies of the records.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Writable)] = C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010 HadoopRDD[4] at sequenceFile at <console>:30\r\n",
       "res8: rdd.type = C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010 HadoopRDD[4] at sequenceFile at <console>:30\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc\n",
    "  .sequenceFile(\"C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010\", classOf[LongWritable], classOf[Writable])\n",
    "\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map{case (k, v) => k.get()}.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:34\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks.map{case(offset,value)=>value.toString.trim()}.filter(value => value.length!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = C:/Users/welcome/Downloads/data-00007-of-00010/data-00007-of-00010 NewHadoopRDD[0] at newAPIHadoopFile at <console>:30\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = wikilinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(String, Array[Byte])] = Array((0,Array(85, 82, 76, 9, 102, 116, 112, 58, 47, 47, 49, 57, 52, 46, 52, 52, 46, 57, 56, 46, 52, 50, 47, 85, 80, 76, 79, 65, 68, 47, 83, 79, 70, 84, 47, 100, 101, 108, 112, 104, 105, 47, 65, 70, 111, 114, 103, 101, 46, 78, 69, 84, 37, 50, 48, 70, 114, 97, 109, 101, 119, 111, 114, 107, 45, 50, 46, 50, 46, 51, 47, 82, 101, 108, 101, 97, 115, 101, 47, 65, 70, 111, 114, 103, 101, 46, 73, 109, 97, 103, 105, 110, 103, 46, 120, 109, 108, 10, 77, 69, 78, 84, 73, 79, 78, 9, 66, 97, 121, 101, 114, 32, 102, 105, 108, 116, 101, 114, 9, 56, 54, 51, 48, 52, 52, 9, 104, 116, 116, 112, 58, 47, 47, 101, 110, 46, 119, 105, 107, 105, 112, 101, 100, 105, 97, 46, 111, 114, 103, 47, 119, 105, 107, 105, 47, 66, 97, 121, 101, 114, 95, 102, 105, 108, 116, 101, 114, 10, 8..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.map { case (text, bytes) => (text.toString, bytes.copyBytes) }.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 16:19:46 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "java.io.NotSerializableException: org.apache.hadoop.io.LongWritable\n",
      "Serialization stack:\n",
      "\t- object not serializable (class: org.apache.hadoop.io.LongWritable, value: 372)\n",
      "\t- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)\n",
      "\t- object (class scala.Tuple2, (372,\n",
      "URL\tftp://202.38.89.18/incoming/ASME/data/pdfs/trk-18/IMECE2011-63321.pdf\n",
      "MENTION\tstarter\t14667\thttp://en.wikipedia.org/wiki/Starter_motor\n",
      "TOKEN\tposition\t19094\n",
      "TOKEN\tup\t19610\n",
      "TOKEN\tinherently\t50290\n",
      "TOKEN\tpotion\t87557\n",
      "TOKEN\taway\t80271\n",
      "TOKEN\taccomplished\t26223\n",
      "TOKEN\tover\t117210\n",
      "TOKEN\tEmerging\t157103\n",
      "TOKEN\tdepending\t118387\n",
      "TOKEN\tpresent\t144887))\n",
      "\t- element of array (index: 0)\n",
      "\t- array (class [Lscala.Tuple2;, size 2)\n",
      "\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:393)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "2020-04-04 16:19:47 ERROR TaskSetManager:70 - Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.io.LongWritable\n",
      "Serialization stack:\n",
      "\t- object not serializable (class: org.apache.hadoop.io.LongWritable, value: 372)\n",
      "\t- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)\n",
      "\t- object (class scala.Tuple2, (372,\n",
      "URL\tftp://202.38.89.18/incoming/ASME/data/pdfs/trk-18/IMECE2011-63321.pdf\n",
      "MENTION\tstarter\t14667\thttp://en.wikipedia.org/wiki/Starter_motor\n",
      "TOKEN\tposition\t19094\n",
      "TOKEN\tup\t19610\n",
      "TOKEN\tinherently\t50290\n",
      "TOKEN\tpotion\t87557\n",
      "TOKEN\taway\t80271\n",
      "TOKEN\taccomplished\t26223\n",
      "TOKEN\tover\t117210\n",
      "TOKEN\tEmerging\t157103\n",
      "TOKEN\tdepending\t118387\n",
      "TOKEN\tpresent\t144887))\n",
      "\t- element of array (index: 0)\n",
      "\t- array (class [Lscala.Tuple2;, size 2); not retrying\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.io.LongWritable\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.io.LongWritable\r",
      "Serialization stack:\r",
      "\t- object not serializable (class: org.apache.hadoop.io.LongWritable, value: 372)\r",
      "\t- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)\r",
      "\t- object (class scala.Tuple2, (372,\r",
      "URL\tftp://202.38.89.18/incoming/ASME/data/pdfs/trk-18/IMECE2011-63321.pdf\r",
      "MENTION\tstarter\t14667\thttp://en.wikipedia.org/wiki/Starter_motor\r",
      "TOKEN\tposition\t19094\r",
      "TOKEN\tup\t19610\r",
      "TOKEN\tinherently\t50290\r",
      "TOKEN\tpotion\t87557\r",
      "TOKEN\taway\t80271\r",
      "TOKEN\taccomplished\t26223\r",
      "TOKEN\tover\t117210\r",
      "TOKEN\tEmerging\t157103\r",
      "TOKEN\tdepending\t118387\r",
      "TOKEN\tpresent\t144887))\r",
      "\t- element of array (index: 0)\r",
      "\t- array (class [Lscala.Tuple2;, size 2)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r",
      "  at scala.Option.foreach(Option.scala:257)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r",
      "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1337)\r",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "wikilinks.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de: org.apache.spark.rdd.RDD[org.apache.hadoop.io.Text] = MapPartitionsRDD[3] at map at <console>:34\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val de = test.map(ele => ele._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 1089244\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "32: error: value KeyBy is not a member of org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)]\r",
     "output_type": "error",
     "traceback": [
      "<console>:32: error: value KeyBy is not a member of org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)]\r",
      "       val n = wikilinks.KeyBy{x => x(0)}\r",
      "                         ^",
      ""
     ]
    }
   ],
   "source": [
    "val n = test.KeyBy{x => x(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Class[_ <: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)]] = class org.apache.spark.rdd.NewHadoopRDD\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.getClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://Shariq:4043\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = local[*], app id = local-1585987906039)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-04 13:41:42 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "2020-04-04 13:41:45 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n",
      "2020-04-04 13:41:45 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\r\n",
      "2020-04-04 13:41:45 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 2\r\n",
       "y: Int = 3\r\n",
       "res0: Int = 5\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2\n",
    "val y = 3\n",
    "x+y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
